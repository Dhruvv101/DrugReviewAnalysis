{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8158279,"sourceType":"datasetVersion","datasetId":4826331},{"sourceId":8233151,"sourceType":"datasetVersion","datasetId":4882799},{"sourceId":8233401,"sourceType":"datasetVersion","datasetId":4882977},{"sourceId":8290114,"sourceType":"datasetVersion","datasetId":4924522}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-05-07T08:58:19.439712Z","iopub.execute_input":"2024-05-07T08:58:19.440662Z","iopub.status.idle":"2024-05-07T08:58:20.725349Z","shell.execute_reply.started":"2024-05-07T08:58:19.440624Z","shell.execute_reply":"2024-05-07T08:58:20.724190Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/docterm/dtm.pkl\n/kaggle/input/cvstop/cv_stop.pkl\n/kaggle/input/doctermm/dtm2.pkl\n/kaggle/input/preprocc-data/preprocessed_data.pkl\n","output_type":"stream"}]},{"cell_type":"code","source":"# Let's read in our document-term matrix\nimport pandas as pd\nimport pickle\n\ndata = pd.read_pickle('/kaggle/input/doctermm/dtm2.pkl')\ndata","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:20.727289Z","iopub.execute_input":"2024-05-07T08:58:20.727846Z","iopub.status.idle":"2024-05-07T08:58:21.466078Z","shell.execute_reply.started":"2024-05-07T08:58:20.727807Z","shell.execute_reply":"2024-05-07T08:58:21.464759Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"        ago  anxiety  bad  better  control  day  days  did  didnt  doctor  \\\n0         0        0    0       0        0    0     0    0      0       1   \n1         0        0    0       0        0    0     0    0      0       0   \n2         0        0    0       0        0    2     0    0      0       0   \n3         0        0    0       0        0    0     0    0      0       0   \n4         0        0    0       0        0    0     0    0      1       0   \n...     ...      ...  ...     ...      ...  ...   ...  ...    ...     ...   \n213864    0        2    0       0        0    4     0    0      0       0   \n213865    0        0    0       0        0    0     0    0      0       0   \n213866    1        1    1       2        0    1     0    0      1       0   \n213867    0        0    0       0        0    0     0    0      0       0   \n213868    0        0    1       0        0    0     0    0      0       0   \n\n        ...  taking  time  took  week  weeks  weight  went  work  year  years  \n0       ...       0     0     0     0      0       0     0     0     0      0  \n1       ...       0     0     0     0      0       0     0     0     0      0  \n2       ...       0     0     0     0      0       0     0     0     0      0  \n3       ...       0     0     1     0      0       0     1     0     0      0  \n4       ...       0     0     0     0      0       0     0     0     0      0  \n...     ...     ...   ...   ...   ...    ...     ...   ...   ...   ...    ...  \n213864  ...       1     1     1     1      0       0     1     1     0      0  \n213865  ...       0     0     0     0      0       0     0     1     0      0  \n213866  ...       0     1     1     0      0       0     0     1     0      1  \n213867  ...       0     0     0     0      0       0     0     0     0      0  \n213868  ...       0     0     0     0      0       0     0     0     0      1  \n\n[213869 rows x 42 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ago</th>\n      <th>anxiety</th>\n      <th>bad</th>\n      <th>better</th>\n      <th>control</th>\n      <th>day</th>\n      <th>days</th>\n      <th>did</th>\n      <th>didnt</th>\n      <th>doctor</th>\n      <th>...</th>\n      <th>taking</th>\n      <th>time</th>\n      <th>took</th>\n      <th>week</th>\n      <th>weeks</th>\n      <th>weight</th>\n      <th>went</th>\n      <th>work</th>\n      <th>year</th>\n      <th>years</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>213864</th>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>213865</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>213866</th>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>213867</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>213868</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>213869 rows × 42 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Import the necessary modules for LDA with gensim\n# Terminal / Anaconda Navigator: conda install -c conda-forge gensim\nfrom gensim import matutils, models\nimport scipy.sparse\n\n# import logging\n# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:21.467751Z","iopub.execute_input":"2024-05-07T08:58:21.468288Z","iopub.status.idle":"2024-05-07T08:58:35.554889Z","shell.execute_reply.started":"2024-05-07T08:58:21.468242Z","shell.execute_reply":"2024-05-07T08:58:35.553635Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# tdm.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:35.557507Z","iopub.execute_input":"2024-05-07T08:58:35.558128Z","iopub.status.idle":"2024-05-07T08:58:35.986019Z","shell.execute_reply.started":"2024-05-07T08:58:35.558084Z","shell.execute_reply":"2024-05-07T08:58:35.981824Z"},"trusted":true},"execution_count":4,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtdm\u001b[49m\u001b[38;5;241m.\u001b[39mhead()\n","\u001b[0;31mNameError\u001b[0m: name 'tdm' is not defined"],"ename":"NameError","evalue":"name 'tdm' is not defined","output_type":"error"}]},{"cell_type":"code","source":"# One of the required inputs is a term-document matrix\ntdm = data.transpose()\ntdm.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:59:05.395986Z","iopub.execute_input":"2024-05-07T08:59:05.396470Z","iopub.status.idle":"2024-05-07T08:59:05.435882Z","shell.execute_reply.started":"2024-05-07T08:59:05.396437Z","shell.execute_reply":"2024-05-07T08:59:05.434544Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"         0       1       2       3       4       5       6       7       \\\nago           0       0       0       0       0       0       0       0   \nanxiety       0       0       0       0       0       0       0       0   \nbad           0       0       0       0       0       0       0       0   \nbetter        0       0       0       0       0       0       0       0   \ncontrol       0       0       0       0       0       0       0       0   \n\n         8       9       ...  213859  213860  213861  213862  213863  213864  \\\nago           0       0  ...       1       0       0       0       0       0   \nanxiety       0       0  ...       0       0       0       0       0       2   \nbad           0       0  ...       0       0       0       0       0       0   \nbetter        0       0  ...       0       0       1       0       0       0   \ncontrol       0       0  ...       0       0       0       0       0       0   \n\n         213865  213866  213867  213868  \nago           0       1       0       0  \nanxiety       0       1       0       0  \nbad           0       1       0       1  \nbetter        0       2       0       0  \ncontrol       0       0       0       0  \n\n[5 rows x 213869 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n      <th>...</th>\n      <th>213859</th>\n      <th>213860</th>\n      <th>213861</th>\n      <th>213862</th>\n      <th>213863</th>\n      <th>213864</th>\n      <th>213865</th>\n      <th>213866</th>\n      <th>213867</th>\n      <th>213868</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ago</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>anxiety</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>bad</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>better</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>control</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 213869 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# The Gensim corpus is a collection of sparse vectors, \n# where each vector represents a document and contains tuples of (word_index, word_frequency) pairs. ","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:35.989472Z","iopub.status.idle":"2024-05-07T08:58:35.990425Z","shell.execute_reply.started":"2024-05-07T08:58:35.990148Z","shell.execute_reply":"2024-05-07T08:58:35.990168Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# We're going to put the term-document matrix into a new gensim format, from df --> sparse matrix --> gensim corpus\nsparse_counts = scipy.sparse.csr_matrix(tdm) # stores large sparse matrix\ncorpus = matutils.Sparse2Corpus(sparse_counts)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:59:08.651096Z","iopub.execute_input":"2024-05-07T08:59:08.651485Z","iopub.status.idle":"2024-05-07T08:59:09.024426Z","shell.execute_reply.started":"2024-05-07T08:59:08.651457Z","shell.execute_reply":"2024-05-07T08:59:09.023157Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":" # Gensim also requires dictionary of the all terms and their respective location in the term-document matrix\ncv = pickle.load(open(\"/kaggle/input/cvstop/cv_stop.pkl\", \"rb\"))\nid2word = dict((v, k) for k, v in cv.vocabulary_.items())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:59:10.298671Z","iopub.execute_input":"2024-05-07T08:59:10.299324Z","iopub.status.idle":"2024-05-07T08:59:10.458320Z","shell.execute_reply.started":"2024-05-07T08:59:10.299291Z","shell.execute_reply":"2024-05-07T08:59:10.456895Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"cv","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:59:12.366536Z","iopub.execute_input":"2024-05-07T08:59:12.366976Z","iopub.status.idle":"2024-05-07T08:59:12.378669Z","shell.execute_reply.started":"2024-05-07T08:59:12.366943Z","shell.execute_reply":"2024-05-07T08:59:12.377238Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"CountVectorizer(stop_words='english')","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer(stop_words=&#x27;english&#x27;)</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Latent Dirichlet Allocation (LDA) topic modeling.** ","metadata":{}},{"cell_type":"markdown","source":"corpus: dtm\nid2word: maps word id to word\npasses: iterations","metadata":{}},{"cell_type":"code","source":"# Now that we have the corpus (term-document matrix) and id2word (dictionary of location: term),\n# we need to specify two other parameters as well - the number of topics and the number of passes\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=10)\nlda.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:59:14.642643Z","iopub.execute_input":"2024-05-07T08:59:14.643064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LDA for num_topics = 4\nlda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=10)\nlda.print_topics()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Topic Modelling(Nouns)","metadata":{}},{"cell_type":"markdown","source":"1. tokenize\n2. pos_tag: part of speech that the text is noun,verb,adjective etc\n3. is_nouns(pos): filters nouns","metadata":{}},{"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\nfrom nltk import word_tokenize, pos_tag\n\ndef nouns(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns.'''\n    is_noun = lambda pos: pos[:2] == 'NN'\n    tokenized = word_tokenize(text)\n    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)] \n    return ' '.join(all_nouns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read in the cleaned data, before the CountVectorizer step\ndata_clean = pd.read_pickle('/kaggle/input/preprocc-data/preprocessed_data.pkl')\ndata_clean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_clean.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_nouns = pd.DataFrame(data_clean['clean_review'].apply(nouns))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from sklearn.feature_extraction import text\n# from sklearn.feature_extraction.text import CountVectorizer\n\n# # Assuming you have already defined the nouns function and loaded the data\n\n# # Apply the nouns function to the transcripts to filter only on nouns\n# data_nouns = pd.DataFrame(data_clean.review_clean.apply(nouns))\n\n# # Re-add the additional stop words since we are recreating the document-term matrix\n# add_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n#                   'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n# stop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# # Convert frozenset to list\n# stop_words = list(stop_words)\n\n# # Recreate a document-term matrix with only nouns\n# cvn = CountVectorizer(stop_words=stop_words)\n# data_cvn = cvn.fit_transform(data_nouns['review_clean'])\n# data_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names())\n# data_dtmn.index = data_nouns.index\n# data_dtmn","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.017541Z","iopub.status.idle":"2024-05-07T08:58:36.017977Z","shell.execute_reply.started":"2024-05-07T08:58:36.017778Z","shell.execute_reply":"2024-05-07T08:58:36.017796Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2>creating dtm with only nouns</h2>","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.feature_extraction import text\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Assuming you have already defined the nouns function and loaded the data\n\n# Apply the nouns function to the transcripts to filter only on nouns\ndata_nouns = pd.DataFrame(data_clean.review_clean.apply(nouns))\n\n# Re-add the additional stop words since we are recreating the document-term matrix\nadd_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Convert frozenset to list\nstop_words = list(stop_words)\n\n# Recreate a document-term matrix with only nouns\ncvn = CountVectorizer(stop_words=stop_words)\ndata_cvn = cvn.fit_transform(data_nouns['review_clean'])\ndata_dtmn = pd.DataFrame(data_cvn.toarray(), columns=cvn.get_feature_names_out())\ndata_dtmn.index = data_nouns.index\ndata_dtmn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Transpose the DataFrame\ndata_dtmn_transposed = data_dtmn.transpose()\n\n# Display the transposed DataFrame\nprint(data_dtmn_transposed)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the gensim corpus\ncorpusn = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmn.transpose()))\n\n# Create the vocabulary dictionary\nid2wordn = dict((v, k) for k, v in cvn.vocabulary_.items())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.025589Z","iopub.status.idle":"2024-05-07T08:58:36.026035Z","shell.execute_reply.started":"2024-05-07T08:58:36.025830Z","shell.execute_reply":"2024-05-07T08:58:36.025848Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's start with 2 topics\nldan = models.LdaModel(corpus=corpusn, num_topics=2, id2word=id2wordn, passes=10)\nldan.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.027585Z","iopub.status.idle":"2024-05-07T08:58:36.028019Z","shell.execute_reply.started":"2024-05-07T08:58:36.027817Z","shell.execute_reply":"2024-05-07T08:58:36.027835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try 4 topics\nldan = models.LdaModel(corpus=corpusn, num_topics=4, id2word=id2wordn, passes=10)\nldan.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.029436Z","iopub.status.idle":"2024-05-07T08:58:36.029860Z","shell.execute_reply.started":"2024-05-07T08:58:36.029663Z","shell.execute_reply":"2024-05-07T08:58:36.029682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"extracting both nouns and ajectives","metadata":{}},{"cell_type":"code","source":"# Let's create a function to pull out nouns from a string of text\ndef nouns_adj(text):\n    '''Given a string of text, tokenize the text and pull out only the nouns and adjectives.'''\n    is_noun_adj = lambda pos: pos[:2] == 'NN' or pos[:2] == 'JJ'\n    tokenized = word_tokenize(text)\n    nouns_adj = [word for (word, pos) in pos_tag(tokenized) if is_noun_adj(pos)] \n    return ' '.join(nouns_adj)","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.031062Z","iopub.status.idle":"2024-05-07T08:58:36.031522Z","shell.execute_reply.started":"2024-05-07T08:58:36.031282Z","shell.execute_reply":"2024-05-07T08:58:36.031299Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_nouns_adj = pd.DataFrame(data_clean['clean_review'].apply(nouns_adj))\ndata_nouns_adj","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.032851Z","iopub.status.idle":"2024-05-07T08:58:36.033261Z","shell.execute_reply.started":"2024-05-07T08:58:36.033066Z","shell.execute_reply":"2024-05-07T08:58:36.033084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(data_nouns_adj.columns)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.035586Z","iopub.status.idle":"2024-05-07T08:58:36.036014Z","shell.execute_reply.started":"2024-05-07T08:58:36.035817Z","shell.execute_reply":"2024-05-07T08:58:36.035834Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.037279Z","iopub.status.idle":"2024-05-07T08:58:36.037734Z","shell.execute_reply.started":"2024-05-07T08:58:36.037526Z","shell.execute_reply":"2024-05-07T08:58:36.037544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Create a new document-term matrix using only nouns and adjectives, also remove common words with max_df\n# cvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n# data_cvna = cvna.fit_transform(data_nouns_adj['clean_review'])\n# data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names())\n# data_dtmna.index = data_nouns_adj.index\n# data_dtmna","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.039355Z","iopub.status.idle":"2024-05-07T08:58:36.039789Z","shell.execute_reply.started":"2024-05-07T08:58:36.039599Z","shell.execute_reply":"2024-05-07T08:58:36.039616Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# nouns+adjective, removing most common words","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport pandas as pd\n\n# Define additional stop words\nadd_stop_words = ['like', 'im', 'know', 'just', 'dont', 'thats', 'right', 'people',\n                  'youre', 'got', 'gonna', 'time', 'think', 'yeah', 'said']\n\n# Concatenate additional stop words with the default English stop words\nstop_words = list(text.ENGLISH_STOP_WORDS) + add_stop_words\n\n# Initialize CountVectorizer with stop words and max_df\ncvna = CountVectorizer(stop_words=stop_words, max_df=.8)\n\n# Fit and transform the data using only nouns and adjectives\ndata_cvna = cvna.fit_transform(data_nouns_adj['clean_review'])\n\n# Create a DataFrame from the document-term matrix\ndata_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n\n# Set the index of the DataFrame\ndata_dtmna.index = data_nouns_adj.index\n\n# Display the DataFrame\ndata_dtmna","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.041222Z","iopub.status.idle":"2024-05-07T08:58:36.041643Z","shell.execute_reply.started":"2024-05-07T08:58:36.041449Z","shell.execute_reply":"2024-05-07T08:58:36.041466Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # Transpose the DataFrame\n# data_dtmna_transposed = data_dtmna.transpose()\n\n# # Display the transposed DataFrame\n# print(data_dtmna_transposed)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.043028Z","iopub.status.idle":"2024-05-07T08:58:36.043465Z","shell.execute_reply.started":"2024-05-07T08:58:36.043241Z","shell.execute_reply":"2024-05-07T08:58:36.043258Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create the gensim corpus\ncorpusna = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n\n# Create the vocabulary dictionary\nid2wordna = dict((v, k) for k, v in cvna.vocabulary_.items())","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.045683Z","iopub.status.idle":"2024-05-07T08:58:36.046092Z","shell.execute_reply.started":"2024-05-07T08:58:36.045897Z","shell.execute_reply":"2024-05-07T08:58:36.045914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try 3 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=10)\nldana.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.048723Z","iopub.status.idle":"2024-05-07T08:58:36.049152Z","shell.execute_reply.started":"2024-05-07T08:58:36.048944Z","shell.execute_reply":"2024-05-07T08:58:36.048962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try 4 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=4, id2word=id2wordna, passes=10)\nldana.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.051297Z","iopub.status.idle":"2024-05-07T08:58:36.051753Z","shell.execute_reply.started":"2024-05-07T08:58:36.051556Z","shell.execute_reply":"2024-05-07T08:58:36.051574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's try 3 topics\nldana = models.LdaModel(corpus=corpusna, num_topics=3, id2word=id2wordna, passes=40)\nldana.print_topics()","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.053473Z","iopub.status.idle":"2024-05-07T08:58:36.053896Z","shell.execute_reply.started":"2024-05-07T08:58:36.053699Z","shell.execute_reply":"2024-05-07T08:58:36.053716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# FINAL","metadata":{}},{"cell_type":"code","source":"for idx, topic in ldana.print_topics(-1):\n    print(\"Topic {}: {}\".format(idx + 1, topic))","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.057203Z","iopub.status.idle":"2024-05-07T08:58:36.057658Z","shell.execute_reply.started":"2024-05-07T08:58:36.057439Z","shell.execute_reply":"2024-05-07T08:58:36.057462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's take a look at which topics each transcript contains\ncorpus_transformed = ldana[corpusna]\n\n# Unpack the values from corpus_transformed\ntopics_per_transcript = [[a for (a, b) in doc] for doc in corpus_transformed]\n\n# Combine with transcript indices\ntopics_with_indices = list(zip(topics_per_transcript, data_dtmna.index))\n\n# Print the result\nfor idx, topic in ldana.print_topics(-1):\n     print(\"Topic {}: {}\".format(idx + 1, topic))","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.058772Z","iopub.status.idle":"2024-05-07T08:58:36.059183Z","shell.execute_reply.started":"2024-05-07T08:58:36.058986Z","shell.execute_reply":"2024-05-07T08:58:36.059002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #part 2\n# # Function to tokenize and filter for verbs\n# def verb(text):\n#     '''Given a string of text, tokenize the text and pull out only the verbs.'''\n#     is_verb = lambda pos: pos[:2] == 'VB'\n#     tokenized = word_tokenize(text)\n#     all_verb = [word for (word, pos) in pos_tag(tokenized) if is_verb(pos)]\n#     return ' '.join(all_verb)\n\n# # Apply the nouns function to the transcripts to filter only on nouns\n# data_verb = pd.DataFrame(data_clean['clean_review'].apply(verb))\n# data_verb\n\n# # Create a new document-term matrix using only nouns, adjectives, and verbs, also remove common words with max_df\n# cvna = CountVectorizer(stop_words=list(stop_words), max_df=.8)\n# data_cvna = cvna.fit_transform(data_verb.transcript)\n# data_dtmna = pd.DataFrame(data_cvna.toarray(), columns=cvna.get_feature_names_out())\n# data_dtmna.index = data_verb.index\n\n# # Create the gensim corpus\n# corpus_verb = matutils.Sparse2Corpus(scipy.sparse.csr_matrix(data_dtmna.transpose()))\n\n# # Create the vocabulary dictionary\n# id2word_verb = dict((v, k) for k, v in cvna.vocabulary_.items())\n\n# # Attempt LDA with verbs\n# lda_verb = models.LdaModel(corpus=corpus_verb, num_topics=4, id2word=id2word_verb, passes=10)\n# for idx, topic in ldana.print_topics(-1):\n#     print(\"Topic {}: {}\".format(idx + 1, topic))","metadata":{"execution":{"iopub.status.busy":"2024-05-07T08:58:36.060490Z","iopub.status.idle":"2024-05-07T08:58:36.060918Z","shell.execute_reply.started":"2024-05-07T08:58:36.060719Z","shell.execute_reply":"2024-05-07T08:58:36.060737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}